{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1c503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432aa795",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276b01de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 18:28:16.043579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import sklearn\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,Masking, Dropout, TimeDistributed\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dbf80e",
   "metadata": {},
   "source": [
    "**Vectorizer**\n",
    "\n",
    "- We used a pre-trained vectorizer to store words into vectors and capture semantic relationships between words, allowing the model to understand similarities and differences in meaning.\n",
    "\n",
    "- The vec_model variable now contains the loaded Word2Vec model, which you can use to get word vectors for words in NLP tasks.\n",
    "\n",
    "- The decision to use this vectorizer was made because it is overtrained in Croatian, as can be seen from the files we uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading Croatian vectorizer uploaded on google drive\n",
    "#!gdown --id 1L953pNrGZTiI8vKTXDIUYcGc92VUD9wB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad30fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_model = gensim.models.KeyedVectors.load_word2vec_format('cc.hr.300.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98637e34",
   "metadata": {},
   "source": [
    "**Loading Processed data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34eee22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_dataset.csv') ##training dataset\n",
    "df_dev = pd.read_csv('val_dataset.csv') ##validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09bded",
   "metadata": {},
   "source": [
    "**Feedforward neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85fd829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = df_train[\"Rijeci\"] #input feature\n",
    "train_lemmas = df_train[\"Leme\"] #output feature\n",
    "\n",
    "dev_words = df_dev[\"Rijeci\"] #input feature\n",
    "dev_lemmas = df_dev[\"Leme\"] #output feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e31bd402",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set() # Initializing an empty set\n",
    "\n",
    "def get_vec_form(words, lemmas):\n",
    "    #dictionaries 'x' and 'y' with keys 'form' and 'vec'\n",
    "    x = {'form': [], 'vec': []}\n",
    "    y = {'form': [], 'vec': []}\n",
    "    for w, l in zip(words, lemmas):   #Iterating over pairs of words and lemmas using zip\n",
    "        try:\n",
    "            new_x = vec_model[w] #obtaining the vector representation of the word 'w' from the embedding model\n",
    "        except:\n",
    "            vocab.add(w) \n",
    "            continue\n",
    "        try:\n",
    "            new_y = vec_model[l] #obtaining the vector representation of the lemma 'l' from the embedding model\n",
    "        except:\n",
    "            vocab.add(l)\n",
    "            continue\n",
    "        x['vec'].append(new_x) # Appending the vector representation and corresponding words dictionary\n",
    "        x['form'].append(w)\n",
    "        y['vec'].append(new_y)\n",
    "        y['form'].append(l)\n",
    "        \n",
    "    # Converting the lists of vectors to NumPy arrays\n",
    "    x['vec'] = np.array(x['vec'])\n",
    "    y['vec'] = np.array(y['vec'])\n",
    "    return x, y  \n",
    "# Return the dictionaries 'x' and 'y' containing the vector representations and corresponding words/lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd77a26",
   "metadata": {},
   "source": [
    "Spliting data on train_x, train_y and dev_x, dev_y (validation data) using made function get_vec_form on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf374f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = get_vec_form(train_words, train_lemmas)\n",
    "dev_x, dev_y = get_vec_form(dev_words, dev_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ee0028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308780, 51467)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x['vec']), len(dev_x['vec']) #number of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860e9dd",
   "metadata": {},
   "source": [
    "**Model structure -> Feedforward neural network**\n",
    "\n",
    "**Architecture:**\n",
    "- Input Layer: Dense layer with 512 units and ReLU activation, taking a 300-dimensional input vector.\n",
    "- Dropout Layer: 50% dropout applied after the first layer.\n",
    "- Hidden Layer: Dense layer with 256 units and ReLU activation.\n",
    "- Dropout Layer: 30% dropout applied after the second layer.\n",
    "- Output Layer: Dense layer with 300 units and linear activation, indicating a regression task.\n",
    "\n",
    "**Activation Functions:**\n",
    "- ReLU activation is used in the hidden layers to introduce non-linearity.\n",
    "- Linear activation in the output layer for regression.\n",
    "\n",
    "**Regularization:**\n",
    "- Dropout regularization is applied to mitigate overfitting after the first and second dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1903f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn = Sequential()\n",
    "model_ffnn.add(Dense(512, activation='relu', input_shape=(300,)))\n",
    "model_ffnn.add(Dropout(0.5))\n",
    "model_ffnn.add(Dense(256, activation='relu'))\n",
    "model_ffnn.add(Dropout(0.3))\n",
    "model_ffnn.add(Dense(300, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86001785",
   "metadata": {},
   "source": [
    "**Loss Function:**\n",
    "- The model is compiled with the cosine similarity as the loss function, indicating that the training objective is to maximize the cosine similarity between predicted and true values, suitable for similarity-based tasks.\n",
    "\n",
    "**Optimizer:**\n",
    "- RMSprop is chosen as the optimizer for updating the model weights during training, offering adaptive learning rates that can help converge faster in non-convex optimization problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb9f9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn.compile(loss='cosine_similarity', optimizer=RMSprop())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c0fb6",
   "metadata": {},
   "source": [
    "**Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5ca4421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2413/2413 [==============================] - 11s 4ms/step - loss: -0.6998 - val_loss: -0.7878\n",
      "Epoch 2/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7583 - val_loss: -0.8070\n",
      "Epoch 3/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7703 - val_loss: -0.8162\n",
      "Epoch 4/50\n",
      "2413/2413 [==============================] - 13s 6ms/step - loss: -0.7761 - val_loss: -0.8208\n",
      "Epoch 5/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7799 - val_loss: -0.8241\n",
      "Epoch 6/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7822 - val_loss: -0.8265\n",
      "Epoch 7/50\n",
      "2413/2413 [==============================] - 11s 5ms/step - loss: -0.7840 - val_loss: -0.8281\n",
      "Epoch 8/50\n",
      "2413/2413 [==============================] - 11s 4ms/step - loss: -0.7853 - val_loss: -0.8298\n",
      "Epoch 9/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7864 - val_loss: -0.8312\n",
      "Epoch 10/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7870 - val_loss: -0.8322\n",
      "Epoch 11/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7879 - val_loss: -0.8329\n",
      "Epoch 12/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7885 - val_loss: -0.8340\n",
      "Epoch 13/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7892 - val_loss: -0.8340\n",
      "Epoch 14/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7898 - val_loss: -0.8352\n",
      "Epoch 15/50\n",
      "2413/2413 [==============================] - 9s 4ms/step - loss: -0.7902 - val_loss: -0.8359\n",
      "Epoch 16/50\n",
      "2413/2413 [==============================] - 9s 4ms/step - loss: -0.7906 - val_loss: -0.8359\n",
      "Epoch 17/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7911 - val_loss: -0.8366\n",
      "Epoch 18/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7915 - val_loss: -0.8373\n",
      "Epoch 19/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7920 - val_loss: -0.8375\n",
      "Epoch 20/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7922 - val_loss: -0.8387\n",
      "Epoch 21/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7926 - val_loss: -0.8386\n",
      "Epoch 22/50\n",
      "2413/2413 [==============================] - 11s 5ms/step - loss: -0.7927 - val_loss: -0.8389\n",
      "Epoch 23/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7930 - val_loss: -0.8393\n",
      "Epoch 24/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7933 - val_loss: -0.8397\n",
      "Epoch 25/50\n",
      "2413/2413 [==============================] - 9s 4ms/step - loss: -0.7934 - val_loss: -0.8394\n",
      "Epoch 26/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7938 - val_loss: -0.8399\n",
      "Epoch 27/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7939 - val_loss: -0.8401\n",
      "Epoch 28/50\n",
      "2413/2413 [==============================] - 12s 5ms/step - loss: -0.7942 - val_loss: -0.8406\n",
      "Epoch 29/50\n",
      "2413/2413 [==============================] - 9s 4ms/step - loss: -0.7944 - val_loss: -0.8408\n",
      "Epoch 30/50\n",
      "2413/2413 [==============================] - 9s 4ms/step - loss: -0.7947 - val_loss: -0.8408\n",
      "Epoch 31/50\n",
      "2413/2413 [==============================] - 11s 4ms/step - loss: -0.7949 - val_loss: -0.8409\n",
      "Epoch 32/50\n",
      "2413/2413 [==============================] - 9s 4ms/step - loss: -0.7950 - val_loss: -0.8416\n",
      "Epoch 33/50\n",
      "2413/2413 [==============================] - 9s 4ms/step - loss: -0.7952 - val_loss: -0.8415\n",
      "Epoch 34/50\n",
      "2413/2413 [==============================] - 9s 4ms/step - loss: -0.7953 - val_loss: -0.8419\n",
      "Epoch 35/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7958 - val_loss: -0.8422\n",
      "Epoch 36/50\n",
      "2413/2413 [==============================] - 9s 4ms/step - loss: -0.7959 - val_loss: -0.8422\n",
      "Epoch 37/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7960 - val_loss: -0.8424\n",
      "Epoch 38/50\n",
      "2413/2413 [==============================] - 12s 5ms/step - loss: -0.7961 - val_loss: -0.8426\n",
      "Epoch 39/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7964 - val_loss: -0.8426\n",
      "Epoch 40/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7965 - val_loss: -0.8429\n",
      "Epoch 41/50\n",
      "2413/2413 [==============================] - 11s 5ms/step - loss: -0.7966 - val_loss: -0.8430\n",
      "Epoch 42/50\n",
      "2413/2413 [==============================] - 12s 5ms/step - loss: -0.7968 - val_loss: -0.8432\n",
      "Epoch 43/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7968 - val_loss: -0.8432\n",
      "Epoch 44/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7970 - val_loss: -0.8434\n",
      "Epoch 45/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7969 - val_loss: -0.8433\n",
      "Epoch 46/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7972 - val_loss: -0.8436\n",
      "Epoch 47/50\n",
      "2413/2413 [==============================] - 11s 4ms/step - loss: -0.7973 - val_loss: -0.8436\n",
      "Epoch 48/50\n",
      "2413/2413 [==============================] - 12s 5ms/step - loss: -0.7973 - val_loss: -0.8440\n",
      "Epoch 49/50\n",
      "2413/2413 [==============================] - 10s 4ms/step - loss: -0.7975 - val_loss: -0.8440\n",
      "Epoch 50/50\n",
      "2413/2413 [==============================] - 11s 4ms/step - loss: -0.7976 - val_loss: -0.8439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fcbf1cddff0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ffnn.fit(train_x['vec'], train_y['vec'], validation_data=(dev_x['vec'], dev_y['vec']), \n",
    "               epochs=50, batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b914babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn.save(\"model_ffnn.keras\") #exporting model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122bd1a4",
   "metadata": {},
   "source": [
    "**LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff87fa4",
   "metadata": {},
   "source": [
    "HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73e48a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 50  # Batch size\n",
    "R = 300  # RNN size\n",
    "S = 4   # Max sequence length\n",
    "E = 300  # Embedding size -> dimensionality of the vector space in which words or tokens are represented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d065d8",
   "metadata": {},
   "source": [
    "- Function generated_data generates sequences of word and lemma embeddings for training a LSTM model. Function uses a sliding window approach to create batches of data with a specified batch size (B), sequence length (S), and embedding size (E). \n",
    "- The mode parameter determines whether the function is used for training or testing. If the line_limit is reached, the function stops generating data. The data is yielded in batches as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03c568dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(words, lemmas, vec_model, line_limit=30878, mode='train'):\n",
    "    word_count = 0\n",
    "    line_number = 0\n",
    "\n",
    "    # Initialize arrays to store input (x) and output (y) sequences\n",
    "    x = np.zeros((B, S, E))\n",
    "    y = np.zeros((B, S, E))\n",
    "    \n",
    "    word_seqs = [None for _ in range(B)] # Stores word sequences\n",
    "    lemma_seqs = [None for _ in range(B)] # Stores lemma sequences\n",
    "\n",
    "    word_seq = [] # Current word sequence\n",
    "    lemma_seq = [] # Current lemma sequence\n",
    "    \n",
    "    x_seq = [] # Current x sequence\n",
    "    y_seq = [] # Current y sequence\n",
    "    \n",
    "    i = 0 # Batch index\n",
    "\n",
    "    \n",
    "    # Iterate through words and lemmas\n",
    "    for word, lemma in zip(words, lemmas):\n",
    "        line_number += 1\n",
    "        if line_number > line_limit: # Stopping if line limit is reached\n",
    "            return \n",
    "\n",
    "        # Check if the current sequences have reached the maximum length (S)\n",
    "        if len(x_seq) == S and len(y_seq) == S:\n",
    "            # Convert current sequences to arrays and store them in the batch\n",
    "            x[i] = np.array(x_seq)\n",
    "            y[i] = np.array(y_seq)\n",
    "            word_seqs[i] = word_seq[:]\n",
    "            lemma_seqs[i] = lemma_seq[:]\n",
    "\n",
    "            # If in training mode, popping the first element from sequences to shift the window\n",
    "            if mode == 'train':\n",
    "                x_seq.pop(0)\n",
    "                y_seq.pop(0)\n",
    "                word_seq.pop(0)\n",
    "                lemma_seq.pop(0)\n",
    "            else:                   # If not in training mode, reseting the sequences\n",
    "                x_seq = []\n",
    "                y_seq = []\n",
    "                word_seq = []\n",
    "                lemma_seq = []\n",
    "            i += 1\n",
    "\n",
    "            # If the batch is full, yield the data and reset for the next batch\n",
    "            if i >= B:\n",
    "                yield x, y, word_seqs, lemma_seqs\n",
    "                x = np.zeros((B, S, E))\n",
    "                y = np.zeros((B, S, E))\n",
    "                word_seqs = [None for _ in range(B)]\n",
    "                lemma_seqs = [None for _ in range(B)]\n",
    "                i = 0\n",
    "                word_count += S\n",
    "\n",
    "        try:             # Get word and lemma embeddings from the vector model\n",
    "            word_embedding = vec_model[word]\n",
    "            lemma_embedding = vec_model[lemma]\n",
    "        except KeyError:     # If not found, using zero vectors\n",
    "            word_embedding = np.zeros(E)\n",
    "            lemma_embedding = np.zeros(E)\n",
    "\n",
    "       # Appending the embeddings and the words/lemmas to the current sequences\n",
    "        x_seq.append(word_embedding)\n",
    "        y_seq.append(lemma_embedding)\n",
    "        word_seq.append(word)\n",
    "        lemma_seq.append(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ae079",
   "metadata": {},
   "source": [
    "Applying generate_data function on real data\n",
    "W and l in these lists allows access to the word and lemma sequences associated with each batch, which could be useful for evaluation purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481055df",
   "metadata": {},
   "source": [
    "- Loading data again to ensure that is fresh for next model, also reduced 10 times because my laptop can train full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efc3f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = df_train[\"Rijeci\"].values[:30878] #input feature\n",
    "train_lemmas = df_train[\"Leme\"].values[:30878] #output feature\n",
    "\n",
    "dev_words = df_dev[\"Rijeci\"].values[:5154] #input feature\n",
    "dev_lemmas = df_dev[\"Leme\"].values[:5154] #output feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1624f968",
   "metadata": {},
   "source": [
    "(X, Y) -> input and output pairs.\n",
    "(x, y, w, l) -> x is the input, y is the output, w is the words, l is the lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4afaf030",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [(X, Y) for X, Y, _, _ in generate_data(train_words, train_lemmas, vec_model, line_limit=30878)]\n",
    "dev_batches = [(x, y, w, l) for x, y, w, l in generate_data(dev_words, dev_lemmas, vec_model, line_limit=5154, mode='dev')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2dec25",
   "metadata": {},
   "source": [
    "**Model Structure:** Long Short-Term Memory (LSTM) network\n",
    "\n",
    "**Architecture:**\n",
    "- Input Layer: Masking layer applied to sequences with a mask value of 0.0, shaping input data with dimensions (S, E).\n",
    "- LSTM Layer: R units in the LSTM layer configured to return sequences\n",
    "- Dropout Layer: 20% dropout rate applied after the LSTM layer to prevent overfitting\n",
    "- TimeDistributed Dense Layer: Linear activation applied independently to each time step, with E units, indicating the output size for each sequence\n",
    "  \n",
    "**Model Compilation:**\n",
    "- Loss Function: Cosine similarity chosen as the loss function for training\n",
    "- Optimizer: RMSprop utilized as the optimizer for weight updates during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16bc018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMmodel = Sequential()\n",
    "LSTMmodel.add(Masking(mask_value=.0, input_shape=(S, E)))\n",
    "LSTMmodel.add(LSTM(R, return_sequences=True))\n",
    "LSTMmodel.add(Dropout(.2))\n",
    "LSTMmodel.add(TimeDistributed(Dense(E, activation='linear')))\n",
    "LSTMmodel.compile(loss='cosine_similarity', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb62517c",
   "metadata": {},
   "source": [
    "**Running the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18f2197b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 \ttrain loss: -0.6671 \tdev loss: -0.7605\n",
      "epoch: 10 \ttrain loss: -0.8131 \tdev loss: -0.8364\n",
      "epoch: 20 \ttrain loss: -0.8228 \tdev loss: -0.8482\n",
      "epoch: 30 \ttrain loss: -0.8279 \tdev loss: -0.8538\n",
      "epoch: 40 \ttrain loss: -0.8313 \tdev loss: -0.8572\n",
      "epoch: 50 \ttrain loss: -0.8341 \tdev loss: -0.8599\n",
      "epoch: 60 \ttrain loss: -0.8363 \tdev loss: -0.8619\n",
      "epoch: 70 \ttrain loss: -0.8383 \tdev loss: -0.8639\n",
      "epoch: 80 \ttrain loss: -0.8401 \tdev loss: -0.8656\n",
      "epoch: 90 \ttrain loss: -0.8417 \tdev loss: -0.8669\n",
      "epoch: 100 \ttrain loss: -0.8430 \tdev loss: -0.8682\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): #100 epochs\n",
    "    train_loss = 0 # initialization\n",
    "    train_batch_c = 0 #initalization batch count\n",
    "\n",
    "    for X, Y in train_set: # Iterating through training set batches (X: input, Y: target)\n",
    "        train_loss += LSTMmodel.train_on_batch(X, Y)  # Updating training loss and batch count by training on the batch\n",
    "        train_batch_c += 1\n",
    "\n",
    "    #similar process repeated only for validation    \n",
    "    dev_loss = 0\n",
    "    dev_batch_c = 0\n",
    "\n",
    "    for X, Y, _, _ in dev_batches:\n",
    "        dev_loss += LSTMmodel.test_on_batch(X, Y)\n",
    "        dev_batch_c += 1\n",
    "\n",
    "    \n",
    "    # Checking if either training batch count or development batch count is zero    \n",
    "    if train_batch_c == 0 or dev_batch_c == 0:\n",
    "        print('Warning: train_batch_c or dev_batch_c is zero. Skipping epoch', epoch + 1)\n",
    "        continue\n",
    "\n",
    "    # Checking if it is the first epoch or a multiple of 10, showing only 10,20,30 etc..\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        print('epoch:', epoch + 1, \n",
    "              '\\ttrain loss: {0:.4f}'.format(train_loss / train_batch_c), \n",
    "              '\\tdev loss: {0:.4f}'.format(dev_loss / dev_batch_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35ac4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMmodel.save(\"lstm_model.keras\") #exporting model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
