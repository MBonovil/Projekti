{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1c503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432aa795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (5.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gdown) (4.66.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests[socks]->gdown) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests[socks]->gdown) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests[socks]->gdown) (2023.5.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276b01de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 06:51:27.150543: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import sklearn\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,Masking, Dropout, TimeDistributed\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dbf80e",
   "metadata": {},
   "source": [
    "Vectorizer\n",
    "\n",
    "We used a pre-trained vectorizer to store words into vectors and capture semantic relationships between words, allowing the model to understand similarities and differences in meaning.\n",
    "\n",
    "The vec_model variable now contains the loaded Word2Vec model, which you can use to get word vectors for words in NLP tasks.\n",
    "\n",
    "The decision to use this vectorizer was made because it is overtrained in Croatian, as can be seen from the files we uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eea4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading Croatian vectorizer uploaded on google drive\n",
    "#!gdown --id 1L953pNrGZTiI8vKTXDIUYcGc92VUD9wB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad30fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_model = gensim.models.KeyedVectors.load_word2vec_format('cc.hr.300.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98637e34",
   "metadata": {},
   "source": [
    "Loading Processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34eee22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_dataset.csv') ##training dataset\n",
    "df_dev = pd.read_csv('val_dataset.csv') ##validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09bded",
   "metadata": {},
   "source": [
    "Feedforward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85fd829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = df_train[\"Rijeci\"] #input feature\n",
    "train_lemmas = df_train[\"Leme\"] #output feature\n",
    "\n",
    "dev_words = df_dev[\"Rijeci\"] #input feature\n",
    "dev_lemmas = df_dev[\"Leme\"] #output feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e31bd402",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set() # Initializing an empty set\n",
    "\n",
    "def get_vec_form(words, lemmas):\n",
    "    #dictionaries 'x' and 'y' with keys 'form' and 'vec'\n",
    "    x = {'form': [], 'vec': []}\n",
    "    y = {'form': [], 'vec': []}\n",
    "    for w, l in zip(words, lemmas):   #Iterating over pairs of words and lemmas using zip\n",
    "        try:\n",
    "            new_x = vec_model[w] #obtaining the vector representation of the word 'w' from the embedding model\n",
    "        except:\n",
    "            vocab.add(w) \n",
    "            continue\n",
    "        try:\n",
    "            new_y = vec_model[l] #obtaining the vector representation of the lemma 'l' from the embedding model\n",
    "        except:\n",
    "            vocab.add(l)\n",
    "            continue\n",
    "        x['vec'].append(new_x) # Appending the vector representation and corresponding words dictionary\n",
    "        x['form'].append(w)\n",
    "        y['vec'].append(new_y)\n",
    "        y['form'].append(l)\n",
    "        \n",
    "    # Converting the lists of vectors to NumPy arrays\n",
    "    x['vec'] = np.array(x['vec'])\n",
    "    y['vec'] = np.array(y['vec'])\n",
    "    return x, y  \n",
    "# Return the dictionaries 'x' and 'y' containing the vector representations and corresponding words/lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a6cf1",
   "metadata": {},
   "source": [
    "Spliting data on train_x, train_y and dev_x, dev_y (validation data) using made function get_vec_form on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cf374f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = get_vec_form(train_words, train_lemmas)\n",
    "dev_x, dev_y = get_vec_form(dev_words, dev_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "30ee0028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308780, 51548)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x['vec']), len(dev_x['vec']) #number of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33f5fd",
   "metadata": {},
   "source": [
    "Model structure -> Feedforward neural network\n",
    "\n",
    "Architecture:\n",
    "Input Layer: Dense layer with 512 units and ReLU activation, taking a 300-dimensional input vector.\n",
    "Dropout Layer: 50% dropout applied after the first layer.\n",
    "Hidden Layer: Dense layer with 256 units and ReLU activation.\n",
    "Dropout Layer: 30% dropout applied after the second layer.\n",
    "Output Layer: Dense layer with 300 units and linear activation, indicating a regression task.\n",
    "Activation Functions:\n",
    "ReLU activation is used in the hidden layers to introduce non-linearity.\n",
    "Linear activation in the output layer for regression.\n",
    "Regularization:\n",
    "Dropout regularization is applied to mitigate overfitting after the first and second dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1903f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn = Sequential()\n",
    "model_ffnn.add(Dense(512, activation='relu', input_shape=(300,)))\n",
    "model_ffnn.add(Dropout(0.5))\n",
    "model_ffnn.add(Dense(256, activation='relu'))\n",
    "model_ffnn.add(Dropout(0.3))\n",
    "model_ffnn.add(Dense(300, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eb9f9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn.compile(loss='cosine_similarity', optimizer=RMSprop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f5ca4421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fe2830f21d0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ffnn.fit(train_x['vec'], train_y['vec'], validation_data=(dev_x['vec'], dev_y['vec']), \n",
    "               epochs=50, batch_size=128, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b914babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn.save(\"model_ffnn.keras\") #exporting model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122bd1a4",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff87fa4",
   "metadata": {},
   "source": [
    "HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e48a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 50  # Batch size\n",
    "R = 300  # RNN size\n",
    "S = 4   # Max sequence length\n",
    "E = 300  # Embedding size -> dimensionality of the vector space in which words or tokens are represented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d065d8",
   "metadata": {},
   "source": [
    "Function generated_data generates sequences of word and lemma embeddings for training a LSTM model. Function uses a sliding window approach to create batches of data with a specified batch size (B), sequence length (S), and embedding size (E). The mode parameter determines whether the function is used for training or testing. If the line_limit is reached, the function stops generating data. The data is yielded in batches as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c568dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(words, lemmas, vec_model, line_limit=308780, mode='train'):\n",
    "    word_count = 0\n",
    "    line_number = 0\n",
    "\n",
    "    # Initialize arrays to store input (x) and output (y) sequences\n",
    "    x = np.zeros((B, S, E))\n",
    "    y = np.zeros((B, S, E))\n",
    "    \n",
    "    word_seqs = [None for _ in range(B)] # Stores word sequences\n",
    "    lemma_seqs = [None for _ in range(B)] # Stores lemma sequences\n",
    "\n",
    "    word_seq = [] # Current word sequence\n",
    "    lemma_seq = [] # Current lemma sequence\n",
    "    \n",
    "    x_seq = [] # Current x sequence\n",
    "    y_seq = [] # Current y sequence\n",
    "    \n",
    "    i = 0 # Batch index\n",
    "\n",
    "    \n",
    "    # Iterate through words and lemmas\n",
    "    for word, lemma in zip(words, lemmas):\n",
    "        line_number += 1\n",
    "        if line_number > line_limit: # Stopping if line limit is reached\n",
    "            return \n",
    "\n",
    "        # Check if the current sequences have reached the maximum length (S)\n",
    "        if len(x_seq) == S and len(y_seq) == S:\n",
    "            # Convert current sequences to arrays and store them in the batch\n",
    "            x[i] = np.array(x_seq)\n",
    "            y[i] = np.array(y_seq)\n",
    "            word_seqs[i] = word_seq[:]\n",
    "            lemma_seqs[i] = lemma_seq[:]\n",
    "\n",
    "            # If in training mode, popping the first element from sequences to shift the window\n",
    "            if mode == 'train':\n",
    "                x_seq.pop(0)\n",
    "                y_seq.pop(0)\n",
    "                word_seq.pop(0)\n",
    "                lemma_seq.pop(0)\n",
    "            else:                   # If not in training mode, reseting the sequences\n",
    "                x_seq = []\n",
    "                y_seq = []\n",
    "                word_seq = []\n",
    "                lemma_seq = []\n",
    "            i += 1\n",
    "\n",
    "            # If the batch is full, yield the data and reset for the next batch\n",
    "            if i >= B:\n",
    "                yield x, y, word_seqs, lemma_seqs\n",
    "                x = np.zeros((B, S, E))\n",
    "                y = np.zeros((B, S, E))\n",
    "                word_seqs = [None for _ in range(B)]\n",
    "                lemma_seqs = [None for _ in range(B)]\n",
    "                i = 0\n",
    "                word_count += S\n",
    "\n",
    "        try:             # Get word and lemma embeddings from the vector model\n",
    "            word_embedding = vec_model[word]\n",
    "            lemma_embedding = vec_model[lemma]\n",
    "        except KeyError:     # If not found, using zero vectors\n",
    "            word_embedding = np.zeros(E)\n",
    "            lemma_embedding = np.zeros(E)\n",
    "\n",
    "       # Appending the embeddings and the words/lemmas to the current sequences\n",
    "        x_seq.append(word_embedding)\n",
    "        y_seq.append(lemma_embedding)\n",
    "        word_seq.append(word)\n",
    "        lemma_seq.append(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ae079",
   "metadata": {},
   "source": [
    "Applying generate_data function on real data\n",
    "W and l in these lists allows access to the word and lemma sequences associated with each batch, which could be useful for evaluation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efc3f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = df_train[\"Rijeci\"]\n",
    "train_lemmas = df_train[\"Leme\"]\n",
    "\n",
    "dev_words = df_dev[\"Rijeci\"]\n",
    "dev_lemmas = df_dev[\"Leme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4afaf030",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [(X, Y) for X, Y, _, _ in generate_data(train_words, train_lemmas, vec_model, line_limit=308780)]\n",
    "dev_batches = [(x, y, w, l) for x, y, w, l in generate_data(dev_words, dev_lemmas, vec_model, line_limit=51548, mode='dev')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16bc018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the network\n",
    "LSTMmodel = Sequential()\n",
    "LSTMmodel.add(Masking(mask_value=.0, input_shape=(S, E)))\n",
    "LSTMmodel.add(LSTM(R, return_sequences=True))\n",
    "LSTMmodel.add(Dropout(.2))\n",
    "LSTMmodel.add(TimeDistributed(Dense(E, activation='linear')))\n",
    "LSTMmodel.compile(loss='cosine_similarity', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    train_loss = 0\n",
    "    train_batch_c = 0\n",
    "\n",
    "    for X, Y in train_set:\n",
    "        train_loss += LSTMmodel.train_on_batch(X, Y)\n",
    "        train_batch_c += 1\n",
    "\n",
    "    dev_loss = 0\n",
    "    dev_batch_c = 0\n",
    "\n",
    "    for X, Y, _, _ in dev_batches:\n",
    "        dev_loss += LSTMmodel.test_on_batch(X, Y)\n",
    "        dev_batch_c += 1\n",
    "\n",
    "    if train_batch_c == 0 or dev_batch_c == 0:\n",
    "        print('Warning: train_batch_c or dev_batch_c is zero. Skipping epoch', epoch + 1)\n",
    "        continue\n",
    "\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        print('epoch:', epoch + 1, \n",
    "              '\\ttrain loss: {0:.4f}'.format(train_loss / train_batch_c), \n",
    "              '\\tdev loss: {0:.4f}'.format(dev_loss / dev_batch_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35ac4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMmodel.save(\"lstm_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
